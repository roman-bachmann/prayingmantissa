{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting CSV files to graph-tool graphs\n",
    "\n",
    "In order to not have to convert the csv files to a graph every time we need it, we decided to create the graph once and store it to the disk. Like that we save on computation time when we need to work on the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from graph_tool.all import *\n",
    "from graph_tool.draw import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Both the patents and citations data sheets are several gigabytes large tab-separated files and the categorymap\n",
    "clocks in at about 200MB.\n",
    "As a first step in the cleaning pipeline we resorted to preprocessing via the command in order to\n",
    "reduce size of the files by extracting only the relevant columns.\n",
    "\n",
    "More specifically, the coreutil [awk](https://www.gnu.org/software/gawk/manual/gawk.html) is a handy stream\n",
    "processing language through which a tab-separated file can be read line by line, processed and written to\n",
    "a new file, preferably a comma-separated one.\n",
    "\n",
    "#### Example\n",
    "\n",
    "```shell\n",
    "echo 'Alice\\tBob\\tEve' | awk -F '\\t' '{ print $1\",\"$3 }' > alice_eve.csv\n",
    "cat alice_eve.csv # Alice,Eve```\n",
    "\n",
    "\n",
    "In our case we extract from the above citations_raw.tsv file the patent_id and the citation_id columns, from the\n",
    "patents_raw.tsv the id and data columns, and from the categorymap.tsv the patent_id and subcategory_id columns.\n",
    "\n",
    "In terms of filesize reduction we get the following improvements:\n",
    " - citations_raw.tsv **7.71GB** to **1.45GB** (~81% reduction)\n",
    " - patents_raw.tsv **4.9GB** to **121MB** (~97% reduction)\n",
    " - categorymap_raw.tsv **204.2MB** to **61.3MB** (~70% reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "citations = pd.read_csv(DATA + 'citations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "citations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "citations.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "citations.dropna(inplace=True)\n",
    "\n",
    "citations = citations[citations.patent_id.str.match(r'([D]\\d+$)|(^\\d+$)')]\n",
    "citations = citations[citations.citation_id.str.match(r'([D]\\d+$)|(^\\d+$)')]\n",
    "\n",
    "citations.columns.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "citations['patent_id'] = np.array(list(map(lambda i: int(i, 16), citations.patent_id)), dtype=np.int32)\n",
    "citations['citation_id'] = np.array(list(map(lambda i: int(i, 16), citations.citation_id)), dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "citations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "citations.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving mapped citations to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "citations.to_csv('citations_processed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the graph\n",
    "\n",
    "Vertexes in graph-tool are named after an index from 0 to N-1, where N is the number of vertices in the graph. In order to know which vertex is which, we need to store the id of the respective patent inside a property of each vertex. Because graph-tool has no inbuilt function to create such a graph with our id's, we loop through all edges and add them manually to the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = Graph()\n",
    "props = graph.add_edge_list(citations.values, hashed=True)\n",
    "graph.vertex_properties[\"id\"] = props"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph.vp.id['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pos = arf_layout(graph, max_iter=0)\n",
    "#graph_draw(graph, pos=pos, vertex_text=prop, vertex_font_size=10, inline=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the graph to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph.save(\"citations_graph.xml.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g2 = load_graph(\"citations_graph.xml.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g2.vp.id['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying out some graph algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's plot its in-degree distribution\n",
    "shortest_path(graph, graph.vertex(234), graph.vertex(81273))\n",
    "'{0:0X}'.format(int(graph.vp.id['234']))\n",
    "v = find_vertex(graph, graph.vertex_properties.id, '127345446')\n",
    "v2 = v[0]\n",
    "int(v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vertices = [int(v) for v in graph.vertices()]\n",
    "in_deg = graph.get_in_degrees(vertices)\n",
    "in_deg.max()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
